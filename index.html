<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sathwika Nemurugommula - Portfolio</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation Bar -->
    <header>
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#about">About</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <!-- Home Section -->
    <section id="home" class="section home">
        <div class="container">
            <img src="C:/Users/sathw/OneDrive/Desktop/Portfolio/images/profilepic.jpeg" alt="Profile Picture" class="profile-img">
            <h1>Nemurugommula Sathwika</h1>
            <p>Data Engineer | Cloud Architect | AI Enthusiast</p>
        </div>
    </section>

    <!-- About Section -->
    <section id="about" class="section about">
        <div class="container">
            <h2>About Me</h2>
            <p>I am a highly skilled and results-driven Data Engineer with extensive experience in designing, developing, and maintaining scalable data pipelines and data solutions. With a strong background in cloud technologies such as AWS, Azure, and Snowflake, I have worked on diverse projects spanning across real-time data processing, ETL automation, machine learning integration, and business intelligence.
                
                Throughout my career, I have architected and implemented high-performance ETL workflows using tools like AWS Lambda, Kinesis, Glue, and Redshift, optimizing data pipelines for speed, efficiency, and accuracy. My expertise in machine learning frameworks (TensorFlow, PyTorch, SageMaker) and advanced data processing technologies (PySpark, Apache Kafka, Databricks) has enabled me to lead projects in predictive analytics, sentiment analysis, and real-time reporting.
                
                I am passionate about applying my knowledge to solve complex data challenges, whether it's improving operational workflows, enhancing data analytics capabilities, or creating automated systems that deliver actionable insights. I thrive in environments where innovation is key and enjoy collaborating with cross-functional teams to bring projects to life.
                
                In addition to my technical skills, I possess a strong foundation in cloud architecture, DevOps practices, and data security, ensuring that all systems are built with scalability, reliability, and compliance in mind. Whether it's developing real-time data streams, designing data warehouses, or automating business processes, I am committed to delivering impactful solutions that drive business growth.</p>
        </div>
    </section>

    <!-- Experience Section -->
    <section id="experience" class="section experience">
        <div class="container">
            <h2>Experiences</h2>
            <div class="experience-cards">
                <div class="experience-card">
                    <button class="exp-btn">Evoke Technologies  - April 2016</button>
                    <div class="exp-details">
                        <p><strong>Job Title:</strong> Data Engineer</p>
                        <p><strong>Description:</strong> I supported the development of ETL pipelines using Apache Airflow and assisted in integrating data into a Snowflake data warehouse with Snowpipe. I collaborated on data transformation and cleaning using Python (pandas, NumPy) and contributed to data validation to ensure quality. Additionally, I helped write basic PySpark scripts, deploy containerized workflows with Docker, and monitor pipelines using AWS CloudWatch. I also assisted in setting up data security practices with AWS IAM and participated in team discussions on data governance and architecture.</p>
                    </div>
                </div>

                <div class="experience-card">
                    <button class="exp-btn">Desert Financial Credit Union  - July 2018</button>
                    <div class="exp-details">
                        <p><strong>Job Title:</strong> Data Engineer</p>
                        <p><strong>Description:</strong> I developed and maintained ETL workflows using Azure Data Factory, integrating data from multiple sources for seamless processing. I contributed to large-scale data transformation with Azure Databricks (PySpark, Delta Lake) and optimized storage within Azure Data Lake Gen2. Additionally, I helped set up Azure Synapse Analytics, built Power BI dashboards for real-time reporting, and implemented automated data validation using Azure Functions and Great Expectations. I also worked on incremental loading strategies, optimized serverless SQL queries, and assisted in setting up Azure DevOps CI/CD pipelines for deployment automation.</p>
                    </div>
                </div>

                <div class="experience-card">
                    <button class="exp-btn">Wesco International  - Nov 2019</button>
                    <div class="exp-details">
                        <p><strong>Job Title:</strong> AWS Engineer</p>
                        <p><strong>Description:</strong> Designed and implemented scalable ETL pipelines using AWS Glue and Apache Spark, optimizing data extraction, transformation, and loading across multiple sources. Managed AWS Redshift data warehouses, improving query performance and data modeling for high-volume analytics. Leveraged AWS S3, Lambda, and Step Functions for efficient data storage, automation, and serverless ETL workflows. Integrated real-time data processing with AWS Kinesis and optimized analytics with Amazon Athena, ensuring cost-effective and scalable data solutions. Implemented infrastructure as code (IaC) with CloudFormation and Terraform, while enhancing security and monitoring using AWS IAM and CloudWatch.</p>
                    </div>
                </div>
                <div class="experience-card">
                    <button class="exp-btn">Mphasis - May 2021</button>
                    <div class="exp-details">
                        <p><strong>Job Title:</strong> Data Engineer</p>
                        <p><strong>Description:</strong> Designed and managed ETL pipelines using AWS Glue and Redshift, automating data workflows and integrating multiple sources into a centralized data warehouse. Orchestrated real-time data ingestion with AWS Kinesis and Lambda, while optimizing large-scale processing with Apache Spark on AWS EMR. Implemented scalable storage solutions on S3, automated data validation, and optimized Redshift performance with advanced tuning strategies. Leveraged AWS Step Functions, CloudFormation, and CI/CD pipelines for automation, security, and infrastructure as code, while integrating machine learning models within Amazon SageMaker for predictive analytics.</p>
                    </div>
                </div>
                <div class="experience-card">
                    <button class="exp-btn">NGIC - April 2023</button>
                    <div class="exp-details">
                        <p><strong>Job Title:</strong> AWS Engineer</p>
                        <p><strong>Description:</strong> Designed and implemented scalable ETL pipelines using AWS Glue, S3, and Redshift, automating data ingestion and transformation for improved reporting and analytics. Developed Python and AWS Lambda-based workflows for real-time and batch data processing, optimizing data synchronization across systems. Enhanced data quality and monitoring with AWS Glue Crawlers, CloudWatch, and automated validation checks, ensuring accuracy and efficiency. Built Power BI dashboards for data visualization and implemented security best practices with AWS IAM, while streamlining operations through automated testing and comprehensive documentation..</p>
                    </div>
                </div>
              

                <!-- Add more experience cards as needed -->
            </div>
        </div>
    </section>

    <!-- Skills Section -->
    <section id="skills" class="section skills">
        <div class="container">
            <h2>Skills</h2>
            <p>Here are the key skills I possess:</p>
            <ul>
                <li>Data Engineering</li>
                <li>Cloud Architecture (AWS, Azure, Snowflake)</li>
                <li>ETL Workflow Automation</li>
                <li>Machine Learning (TensorFlow, PyTorch, SageMaker)</li>
                <li>Data Processing (PySpark, Apache Kafka, Databricks)</li>
                <li>Business Intelligence</li>
                <li>Real-time Data Processing</li>
                <li>Data Security & Compliance</li>
                <li>DevOps Practices</li>
            </ul>
        </div>
    </section>

    <!-- Education Section -->
    <section id="education" class="section education">
        <div class="container">
            <h2>Education</h2>
            <ul>
                <li><strong>Master's Degree in Computer Science</strong> - Clark University - 2024</li>
                <li><strong>Certifications:</strong> â€¢ Certification on Data Science job simulation ,Certification on Data Analytics and Visualization Job Simulation</li>
            </ul>
        </div>
    </section>

   <!-- Contact Section -->
<section id="contact">
    <h2>Connect With Me</h2>
    <p>Let's connect! Feel free to check out my professional profiles below.</p>
    
    <div class="contact-links">
        <a href="www.linkedin.com/in/nemurugommulasathwika" target="_blank" class="contact-btn linkedin">LinkedIn</a>
        <a href="https://medium.com/@nemurugommulasathwika" target="_blank" class="contact-btn medium">Medium</a>
    </div>
</section>


    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Sathwika Nemurugommula</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
